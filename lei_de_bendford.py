# -*- coding: utf-8 -*-
"""Lei de Bendford.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h_L0KpmnfwPW97KED74NlrPE_zv4Hc57

# Fraude nas Eleições?

Após escultar o podcast [**Naruhodo #154**](https://www.b9.com.br/98315/naruhodo-154-o-que-e-a-lei-de-benford/), soube que um vídeo ficou popular falando sobre [possíveis fraudes nas eleições de 2014](https://www.youtube.com/watch?v=FgpYrXN00Sw&ab_channel=BrasilParalelo). Motivado pela discussão do episódio, fui mais a fundo no assunto. 
No vídeo, apesar das acusações, o estudo divulgado não é detalhado. Ou seja, a metodologia usada para encontrar os resultados não está disponível (pública). Portanto, não podemos ter a replicabilidade exata dos resultados (ou conferí-los). 

Após algumas leituras sobre a utilização da **Lei de Benford na auditação de eleições** <sup>[[1]](https://web.archive.org/web/20140517120934/http://vote.caltech.edu/sites/default/files/benford_pdf_4b97cc5b5b.pdf) [[2]](https://aip.scitation.org/doi/10.1063/1.166498)</sup>, vi que a sua utilização é no mínimo questionável. Como o vídeo não traz um estudo em sí, apenas os resultados tentarei replicá-los baseando-me apenas na aplicação da Lei de Benford. Os dados analizá-dos serão os das eleições para presidente de 2014. 
___

## 1. O que é a Lei de Benford?
A lei de Benford ou lei do primeiro dígito é uma observação da distribuição de dígitos presente em várias conjuntos de números reais. Simplificadamente, a **lei diz que o primeiro dígito significante é provavelmente pequeno**. Esse [vídeo](https://www.youtube.com/watch?v=XXjlR2OK1kM&ab_channel=Numberphile) do [Numberphille](https://www.youtube.com/channel/UCoxcjq-8xIDTYp3uz647V5A) traz uma ótima explicação sobre o assunto.

Matematicamente, a Lei de Benford diz que probabilidade da ocorrencia do dígito $n$ na primeira posição em uma conjunto de números segue a seguinte equação:

$$
P(n) = \log_{10}\left(1 + \frac{1}{n}\right)
$$

Por exemplo, a distribuição dos primeiro dígito da população de 237 países em Julho de 2010, indica uma distribuição que segue a Lei de Benford.

![Fonte: Wikipedia](https://upload.wikimedia.org/wikipedia/commons/0/0b/Benfords_law_illustrated_by_world%27s_countries_population.png "Fonte: https://en.wikipedia.org/wiki/Benford%27s_law")

### Restrições
Apesar de vários conjuntos de dados seguirem essa distribuição, exitem algumas condições.

- Os dados devem abrangir várias ordens de magnitude de maneira **relativamente** uniforme.
- **Flutuações multiplicativas** geram essa distribuição (sequencia de produtos de probabilidades)

___
# Aplicando a Lei de Benford

Vamos primeiro aplicar a lei em alguns dados públicos e verificar sua eficácia. O passo-a-passo abaixo mostra o que feito com os dados de cada exemplo.
- **Visualização**.
- **Limpar/Filtragem**.
- **Análise**.
- **Resultados**.
- **Métricas**.


Antes de prosseguirmos nas análise, vamos garantir que algumas bibliotecas necessárias estão atualizadas e algumas configurações feitas.
"""

# Atualizar alguns packages

# Configurações de exibição
import pandas as pd


"""## 1. Dados de Eleitores por Município
Primeiro, vamos verificar se a quantidade de eleitores por município segue o padrão descrito na lei de Benford. É uma estimativa razoável, já que a quantidade de eleitores é proporcional a de habitantes. Os dados usados nessa análise foram retirados dos site do [TSE](http://www.tse.jus.br/eleitor/estatisticas-de-eleitorado/quantitativo-do-eleitorado) dia 27 de Outubro de 2018 e correspondem a **quantidade de eleitores por município em 2014**.

Primeiro, vamos **importar os dados do TSE** e dá uma olhada neles.
"""

import pandas as pd

# Importa os dados coletados do TSE
url = "https://raw.githubusercontent.com/Fernandohf/Benford-s-Law/master/eleitorado_municipio_2014.csv"
eleitores = pd.read_csv(url, header=0, encoding='latin-1', sep=";")

def visualizar(df):
    # Mostra as primeiras linhas dos dados
    pass
    # Informações sobre os dados

visualizar(eleitores)

"""Já podemos perceber que temos 5738 de municípios de votação (Incluindo Exterior).
A coluna que nos iteressa é:
 - **QTD_ELEITORES**: Quantidade de eleitores por município.
"""

def filter_and_clean(df, col):
    """
    Filtra e limpa os dados, em seguida mostra o resultado final.
    """
    # Filtra os dados
    df = df[[col]]
    df = df.dropna()

    # Após a limpeza
    print(df.info())
    
    print("\n\n Total:" + str(df[col].astype('float64').sum()) + "\n\n")
    
    return df
  
col = "QTD_ELEITORES"

eleitores = filter_and_clean(eleitores, col)

"""De acordo com o [TSE](http://www.tse.jus.br/imprensa/noticias-tse/2018/Agosto/brasil-tem-147-3-milhoes-de-eleitores-aptos-a-votar-nas-eleicoes-2018), de fato, tivemos 142.822.046 eleitores em 201. Assim, podemos continuar nossa análise.
 
Antes de continuar a análise, vamos dá uma olhada na distribuição dos valores que estamos analisando, **Quantidade de Eleitores**.
"""

import seaborn as sns
import matplotlib.pyplot as plt

def show_dist(df, col):
  with sns.axes_style('darkgrid'):
      f, ax = plt.subplots()
      sns.distplot(df[col].astype('float64'), ax=ax)

      ax.set_ylabel("% Frequência")

  print("Valor mínimo: " + str(df[col].astype('float64').min()))
  print("Valor máximo: " + str(df[col].astype('float64').max()))
  
show_dist(eleitores, col)

"""Os valores possuem distribuição de cauda longa e estão definidos em um intervalo de várias ordens de grandeza $[1, 8782406]$.

Agora vamos escrever uma função que coletar as **distribuições dos dígitos presentes nesses dados**.
"""

# Função que coleta a frequência dos primeiros dígitos.
def fd_freq(df, col):
    """
    Retorna a distribuição dos primeiros dígitos da coluna especificada.

    > df: DataFrame de análise
    > col: Coluna que será analisada.
    < df_freq: DataFrame com os resultados;

    """
    # lista de dígitos 1...9
    digits = [str(d) for d in range(1, 10)]
    columns = ["Ocorrências", "Porcentagem"]
    df_freq = pd.DataFrame(index=digits, columns=columns)

    # Para cada dígito
    for d in digits:
        # Total de observações do dígito na primeira posição
        total = df[col].apply(lambda x: str(x)[0] == d).sum()
        
        # Salvo no df
        df_freq.loc[d, "Ocorrências"] = total
        df_freq.loc[d, "Porcentagem"] = total / df.shape[0]
        
    return df_freq

first_digit_freq = fd_freq(eleitores, col)

"""Hmm.. Esses resultados parecem bem próximos dos previstos na Lei de Benford, **vamos plotar os gráficos**."""

import numpy as np

def result_vs_benford(df, col, ax=None):
    with sns.axes_style("whitegrid"):
      f, ax = plt.subplots()
      # Plot dos valores observados
      x_o = df.index.values.astype(int)
      y_o = df[col].values
      ax.bar(x_o, y_o, label="Valores observados")

      # Plot da Lei de Bendford
      x = np.linspace(1, 9, 100)
      y = np.log10(1 + (1/x))
      ax.plot(x, y, label = "Lei de Benford", c='r')

      ax.set_xlabel("Dígito")
      ax.set_ylabel("% Frequência")
      ax.set_xticks(x_o)
      ax.set_ylim((0, 1))
      ax.set_xlim((0, 10))
      ax.legend(frameon=True)
    
result_vs_benford(first_digit_freq, "Porcentagem")

"""Podemover que os resultados estão bem próximos do descrito pela Lei de Benford. Mas quão próximos podemos definir esses resultados? Para isso, vamos tentear responder a seguinte frase, através das etapas descritas.

**Os dados possuem a distribuição definida pela Lei de Benford?**

- Dados serão **simulados** aplicando a técnica de **bootstrap**, para que possamos criar um intervalo de confiança.
- A **frequência de cada dígitos** será usada como a estatística.
- A **estatística** será definida para cada simulação.
- Por fim, teremos um **intervalo de confiança** para cada frequência dos dígitos.

Muita coisa, não é? Vamos por partes.

Primeiro, definir melhor os dados observados como uma sequência de primeiros dígitos.
"""

# Dados observados
def fd_data(df, col):
    """
    Retorna os dados que correspondem a cada dígito na primeira posição

    > df: DataFrame de análise
    > col: Coluna que será analisada.
    < series_data: DataFrame com os resultados;

    """
    # lista de dígitos 1...9
    columns = ["Dados"]

    series_data = df[col].apply(lambda x: str(x)[0]).copy()
        
    # Salvo no df
    series_data.name= "Dados"
    
    return series_data

first_digit_data = fd_data(eleitores, col)
metrics = first_digit_data.value_counts()/first_digit_data.count()

"""Agora, vamos criar as sequências de ** amostras bootstrap** desses dados e sua **estatística de teste**."""

# Para replicabilidade
np.random.seed(42)

# Cria os bootstraps e calcula as estatísticas.
def bootstrap_replicates(data):
    # Amostragem dos dados obtidos
    bs_sample = pd.Series(np.random.choice(data, len(data)))
    
    # Métrica dos dados
    return bs_sample.value_counts() / bs_sample.count()
  
def estimate_interval(data, samples=10000, conf_int = 0.95):
    # Inicializa os resultados
    digits = [str(d) for d in range(1, 10)]
    bs_df = pd.DataFrame(columns=digits)
    for i in range(samples):
        bs_rep = bootstrap_replicates(data)
        bs_df = bs_df.append(bs_rep, ignore_index=True)
        
    int = bs_df.quantile([(1 - conf_int) / 2, conf_int + (1 - conf_int) / 2], axis=0)
    return int
  
estimate_interval(first_digit_data)

"""### <span style="color:blue">**Os resultados seguem a lei de Benford**!</span>

## 2. Dados dos Votos Válidos por Município
Agora, vamos assumir que o número de votos válidos por município tem um comportamento conforme a lei de Benford. 

Mais uma vez os dados foram coletados diretamento do site [TSE](http://www.tse.jus.br/eleicoes/estatisticas/estatisticas-eleitorais) no dia 27 de Outubro de 2018.

Essa tabela possui outros valores bastante interessantes que ainda exploraremos mais adiantes. Porém, inicialmente, vamos aplicar os mesmo passos anteriores para os votos válidos...
"""

# Importa os dados coletados do TSE
url = "https://raw.githubusercontent.com/Fernandohf/Benford-s-Law/master/aecio_2014_2turn.csv"
votos = pd.read_csv(url, header=0, encoding='latin-1', sep=";")

# Visualização
visualizar(votos)

"""Dessa vez, não temos nenhum valor nulo. Então, vamos apenas filtrar a nossa coluna de interesse.

 - **Votos válidos**: Quantidade de votos válidos naquele município.
"""

# Filtra os dados
col2 = "Votos válidos"

# Após o filtro
eleitores = filter_and_clean(votos, col2)

"""De acordo com o [G1](http://g1.globo.com/politica/eleicoes/2014/apuracao-votos-presidente.html), 105.542.273 foi o número de votos válidos no segundo turno. Então, está tudo OK. 

Continuando a análise...
"""

show_dist(votos, col2)

"""Mais uma vez, os valores possuem uma distribuição de cauda longa e de grande intervalo $[7, 6489779]$.

Assim, calculando as frequencia e plotandos os gráficos.
"""

first_digit_freq_2 = fd_freq(votos, col2)

result_vs_benford(first_digit_freq_2, "Porcentagem")

"""### <span style="color:blue">**Os resultados seguem a lei de Benford**!</span>

## 3. Dados dos Votos Nominais por Cadidato
Até agora estávamos implementando a lei em resultados essencialmente demográficos. Mas agora, vamos aplicá-la diretamente nos números que geraram os resultados das eleições. 

Dessa vez o conjunto de dados sendo analisado é o **número de votos válidos para cada candidato por município no segundo turno da eleição de 2014**. Como temos dois candidatos, vamos ter dois conjuntos de dados.

Mais uma vez, continuando o mesmo passo-a-passo dos outros exemplos.
"""

import pandas as pd

# Importa os dados coletados do TSE
url = "https://raw.githubusercontent.com/Fernandohf/Benford-s-Law/master/aecio_2014_2turn.csv"
aecio = pd.read_csv(url, header=0, encoding='latin-1', sep=";")

# Visualização
visualizar(aecio)

# Colunas
print(aecio.columns)

import pandas as pd

url = "https://raw.githubusercontent.com/Fernandohf/Benford-s-Law/master/dilma_2014_2turn.csv"
dilma = pd.read_csv(url, header=0, encoding='latin-1', sep=";")


# Visualização
visualizar(dilma)

# Colunas
print(dilma.columns)

"""A coluna de interesse.

 - **Votos nominais**: Quantidade de votos válidos para o cadidato.
"""

# Filtra os dados
col3 = "Votos nominais"


# Após o filtro
aecio = filter_and_clean(aecio, col3)

dilma = filter_and_clean(dilma, col3)

"""Os dados batem com os divulgados pela imprensa <sup>[[3](http://g1.globo.com/politica/eleicoes/2014/apuracao-votos-presidente.html)]</sup>.
- **Aécio:** 51041155 votos.
- **Dilma:** 54501118 votos.


Continuando a análise...
"""

# Distribuições
show_dist(aecio, col3)

show_dist(dilma, col3)

"""Aqui os resultados são interessantes. Ambos os valores parecem ser uma distribuição de cauda longa, além de intervalo que varia na mesma ordem de grandeza $A\in[5, 4142529], D\in[1, 234725]$.

Por fim, calculando as frequencia e plotandos os gráficos.
"""

# Aécio
first_digit_freq_3 = fd_freq(aecio, col3)

result_vs_benford(first_digit_freq_3, "Porcentagem")

# Dilma
first_digit_freq_4 = fd_freq(dilma, col3)

result_vs_benford(first_digit_freq_4, "Porcentagem")

"""Hmmm, esses resultados parecem bem próximos do descrito pela lei de Benford. Mas pelas declarações do vídeo inicial, estáva esperando uns resultados mais divergente. Vamos criar umas funções que analisar o"""

